{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CHARACTER MODELLING USING LSTM/RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The Model takes the text file as input and trains the RNN that learns to predict the next character in a sequence\n",
    "\n",
    "+ This RNN can be used to generate text character by  character that will look like the original training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Generation Using LSTM/RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader:\n",
    "\n",
    "+ The following cell is the class that helps to read datafrom input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "batch_size = 60\n",
    "num_epochs = 50\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "rnn_size = 128\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Input file, and print a part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-18 04:22:33 URL:https://public.boxcloud.com/d/1/b1!dHcL0sd815gxFj5dDf6XjcLrbPNJjmnGG4Pg5j9FzN8FhTyMhTS29TM0yBI44-VP8oLYeiJsbrHyMaNnbcX7ejNLdNWlJUa-IQvsOIosuKdJzLamJKv6bgjc0WJKQG4yiH9M97PTxnEq01x19DNju3hWFJzyL29K26CB6qm-qeFqCy34mVaQMPhiERx03FLRS4g7lfkzKBBO8xiG8JYou3g1r2cjerTl3s_Ylvj2UZny-Rq3Gf3P-iX6PKrb_Om_dsYt2mHrvZZA6FQvJRyHQvM0TFcA3IPalOr2qUmkAvoDrfZyuvRCWA7LfeHWmV74OaIqW0gISzT6AMVVKakJJMEHP9Bh20K0jSCn4J5YxyUCca3MxhnbYWI6ad6xFax8gnJz_tpDWsWpmaDpDyHnfMezkxRxZrrwWmh64oEuKPGsFmaurz8XMiXQyzaYtTtzVG6XqpfsB_w33cHnaf3n7bljqihHsvOa5kgZJ4j5YLCwHlln9QYQSol4dnkIfxd2CV2oXAIzGfdzyKC41KE6nAu7TU0ERRr6pR0GtX02UM00wuxUuiLNl6K3ThRbn20so39UXGcZcphCdGEzQBoquMpFupEUlrG8USUoLJ5hAyyCEM5ZyB_6_4eKz1oT4brGVxMjAYNUq9ltWfs6cgz6kxBnPhl81l_4oY14oGoq7CyRFm63KvMeb5u9Z_jy1r9VsxxN8P9pbRjeTD30LDf7-iXLYsOw3kIHiSeN3gj_OS0mMEbN7pyLhTlABvC0p9Y41_7KSO8Skur09aeFWCJH63TPfI4urmR3_Y19bomHzPoMp_nDZ6_rdENdTGa7jp-6Tscc4ZFWxBZ7xHOzb0_uuXrTCbx8wC4FKzgYIC1d7FJ5ACTJnZbQSInyK-D105dShRtReKfSqjp8tnQNXBlTv12c1ADACEJVUeZbXyopYyNGnxSGJhVVzh9__ycHjt0EWdX4Dzr7cP2k8nIg2_bN6p3j4cfcdhMhQJjKram4cGK1HxkXUtwh40XawPFiObgMNWEnQgJZY7fs8xfTuBilrUvbsqX2ZC8WrHdFsjeBOb3MTH0EUQ7aFFKshywrPVXk1V0K7DmSmlkL4oCYT4hP2ZDm0VcY_4ZirNHbfeooC7_QrlTJxdYo6uVJdp-gTkKnbFfOmKk7sriYNRJYDcZJYqg6m0SWR7Odf-qZMwXakbIiwnTUGOkiN5lmBRrYS0XdfMEY6f19ZoYcTjpoghDtiTAptVCyIp5AS9Ui-neTNUFk8ddV9wlKrw038_Gpm7M1fGrIl1QKfXEXw8nLWEIDbL-ivUsOdH-aTIfAMv9CoZFEU0LfWoJVq_8p19V1uT44tCfHOGYitof93xZfQiMAtQxv-XNBeW4Qq-akT7vc-UifrBZd1BJlBGVABK63F82vwDQ0uaA6i6l6pdzc9s0CwCSN1wFyrisOacArguJp/download [1115393/1115393] -> \"input.txt\" [1]\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt \n",
    "with open('input.txt', 'r') as f:\n",
    "    read_data = f.read()\n",
    "    print (read_data[0:100])\n",
    "f.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Read data as batches, using TextLoader Class.\n",
    "\n",
    "+ It will convert the character to numbers and represent each sequence as a vector in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "vocabulary size: 65\n",
      "Characters: (' ', 'e', 't', 'o', 'a', 'h', 's', 'r', 'n', 'i', '\\n', 'l', 'd', 'u', 'm', 'y', ',', 'w', 'f', 'c', 'g', 'I', 'b', 'p', ':', '.', 'A', 'v', 'k', 'T', \"'\", 'E', 'O', 'N', 'R', 'S', 'L', 'C', ';', 'W', 'U', 'H', 'M', 'B', '?', 'G', '!', 'D', '-', 'F', 'Y', 'P', 'K', 'V', 'j', 'q', 'x', 'z', 'J', 'Q', 'Z', 'X', '3', '&', '$')\n",
      "vocab number of 'F': 49\n",
      "Character sequences (first batch): [[49  9  7 ...  1  4  7]\n",
      " [19  4 14 ... 14  9 20]\n",
      " [ 8 20 10 ...  8 10 18]\n",
      " ...\n",
      " [21  2  0 ...  0 21  0]\n",
      " [ 9  7  7 ...  0  2  3]\n",
      " [ 3  7  0 ...  5  9 23]]\n"
     ]
    }
   ],
   "source": [
    "data_loader = TextLoader('', batch_size, seq_length)\n",
    "vocab_size = data_loader.vocab_size\n",
    "print (\"vocabulary size:\" ,data_loader.vocab_size)\n",
    "print (\"Characters:\" ,data_loader.chars)\n",
    "print (\"vocab number of 'F':\",data_loader.vocab['F'])\n",
    "print (\"Character sequences (first batch):\", data_loader.x_batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ...,\n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = data_loader.next_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
       "       [ 4, 14, 22, ...,  9, 20,  5],\n",
       "       [20, 10, 29, ..., 10, 18,  4],\n",
       "       ...,\n",
       "       [ 2,  0,  6, ..., 21,  0,  6],\n",
       "       [ 7,  7,  4, ...,  2,  3,  0],\n",
       "       [ 7,  0, 33, ...,  9, 23,  0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 50)\n",
      "(60, 50)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Architecture\n",
    "\n",
    "Each LSTM cell has 5 parts:\n",
    "\n",
    "+ Input\n",
    "+ prv_state\n",
    "+ prv_output\n",
    "+ new_state\n",
    "+ new_output\n",
    "\n",
    "Num of Layers =2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Satcked RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State variable keeps output and new_state of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
    "\n",
    "+ \"BasicRNNCell.zero_state(batch_size, dtype)\" will return zero filled zero tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = stacked_cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState/zeros:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState_1/zeros:0' shape=(60, 128) dtype=float32>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Value of input Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ...,\n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "feed_dict = {input_data : x, targets : y}\n",
    "\n",
    "session.run(input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnnlm', reuse=False):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    \n",
    "        \n",
    "    # embedding variable is initialized randomely\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "\n",
    "\n",
    "    \n",
    "    em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    \n",
    "    \n",
    "    inputs = tf.split(em, seq_length, 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1039549 , -0.08005431, -0.16920386, ..., -0.05346642,\n",
       "         0.13948037, -0.06619333],\n",
       "       [ 0.04570121,  0.16956244,  0.06341098, ...,  0.10946678,\n",
       "         0.01900171, -0.03551383],\n",
       "       [ 0.09159268, -0.05950297,  0.02533752, ..., -0.13935378,\n",
       "         0.09763466, -0.02779722],\n",
       "       ...,\n",
       "       [ 0.13641317, -0.13807559,  0.05002356, ..., -0.08333156,\n",
       "        -0.05852745,  0.1285715 ],\n",
       "       [ 0.09111233,  0.09306572,  0.02881256, ...,  0.08099751,\n",
       "        -0.02222961,  0.07656036],\n",
       "       [ 0.05359456,  0.16265638,  0.05553299, ...,  0.09978445,\n",
       "        -0.05299493,  0.02511995]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "session.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03555945, -0.05591494, -0.08674122, ..., -0.15841456,\n",
       "        -0.01403545, -0.07003055],\n",
       "       [ 0.06486641,  0.13916008,  0.1591173 , ...,  0.14497267,\n",
       "         0.02352007, -0.03990957],\n",
       "       [ 0.1573479 ,  0.10765173,  0.07542165, ...,  0.01938517,\n",
       "         0.13281645,  0.02505915],\n",
       "       ...,\n",
       "       [ 0.04570121,  0.16956244,  0.06341098, ...,  0.10946678,\n",
       "         0.01900171, -0.03551383],\n",
       "       [ 0.09691967, -0.12097788, -0.10015451, ..., -0.05966448,\n",
       "         0.07435201, -0.00315626],\n",
       "       [ 0.1573479 ,  0.10765173,  0.07542165, ...,  0.01938517,\n",
       "         0.13281645,  0.02505915]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "emp = session.run(em, feed_dict = {input_data:x})\n",
    "emp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 50, 128)\n"
     ]
    }
   ],
   "source": [
    "print(em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 50, 128)\n"
     ]
    }
   ],
   "source": [
    "print(emp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_2:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_4:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.split(em, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feeding a batch of 50 Sequence to RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03555945, -0.05591494, -0.08674122, ..., -0.15841456,\n",
       "        -0.01403545, -0.07003055],\n",
       "       [ 0.09789135,  0.17524461, -0.078011  , ..., -0.16016895,\n",
       "        -0.07557131,  0.00349143],\n",
       "       [-0.055346  , -0.08602604,  0.07792567, ...,  0.17299135,\n",
       "         0.07704233, -0.1564047 ],\n",
       "       ...,\n",
       "       [-0.12628981, -0.04801689, -0.05817143, ..., -0.01046297,\n",
       "        -0.10838394, -0.16038889],\n",
       "       [ 0.06486641,  0.13916008,  0.1591173 , ...,  0.14497267,\n",
       "         0.02352007, -0.03990957],\n",
       "       [ 0.01414138,  0.13014899,  0.08713482, ..., -0.06855214,\n",
       "         0.03155929,  0.02914608]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0], feed_dict = {input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the new state and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_98:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_99:0' shape=(60, 128) dtype=float32>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n",
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_5:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_7:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_9:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output network after feeding it with first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04861006,  0.03924445, -0.04854872, ...,  0.01310559,\n",
       "        -0.04325602,  0.02226839],\n",
       "       [ 0.14082868, -0.09441803,  0.0839439 , ...,  0.04821848,\n",
       "        -0.06481316, -0.06492446],\n",
       "       [ 0.01229214, -0.02644767,  0.01852591, ...,  0.02330318,\n",
       "         0.04176168, -0.00712036],\n",
       "       ...,\n",
       "       [ 0.0199025 , -0.09378577, -0.05991625, ..., -0.04106001,\n",
       "         0.02913299,  0.06101136],\n",
       "       [ 0.04954085, -0.03680451,  0.03716031, ..., -0.08969241,\n",
       "         0.04023373,  0.04445355],\n",
       "       [-0.10488816, -0.07871149, -0.01299806, ..., -0.07100566,\n",
       "        -0.04919834, -0.06924637]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_output = outputs[0]\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "session.run(first_output, feed_dict = {input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Probability using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(3000, 128) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probability of next character in all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01475394, 0.01829847, 0.02017664, ..., 0.01643569, 0.01166173,\n",
       "        0.01352268],\n",
       "       [0.01321734, 0.01589874, 0.01485914, ..., 0.01585357, 0.0113903 ,\n",
       "        0.0167792 ],\n",
       "       [0.01426184, 0.01513404, 0.01405699, ..., 0.0125112 , 0.01292378,\n",
       "        0.01315115],\n",
       "       ...,\n",
       "       [0.0209191 , 0.02091805, 0.02358138, ..., 0.01521501, 0.01223968,\n",
       "        0.01203389],\n",
       "       [0.0175853 , 0.01975196, 0.01607215, ..., 0.01122846, 0.01189314,\n",
       "        0.01257226],\n",
       "       [0.01367688, 0.015186  , 0.01166135, ..., 0.0102095 , 0.01622624,\n",
       "        0.01499231]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(probs, feed_dict = {input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the cost of training with Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_clip = 5\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self,sample=False):\n",
    "        rnn_size = 128 \n",
    "        batch_size = 60\n",
    "        seq_length = 50\n",
    "        num_layers = 2\n",
    "        vocab_size = 65\n",
    "        grad_clip = 5.\n",
    "        if sample:\n",
    "            print(\">> sample mode:\")\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        \n",
    "        basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.initial_state = stacked_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        with tf.variable_scope('rnnlm_class1'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        # The value of state is updated after processing each batch of chars.\n",
    "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([batch_size * seq_length])],\n",
    "                vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    \n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
    "        #print state\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating LSTM Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"rnn\"):\n",
    "    model = LSTMModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train using LSTM Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model through feeding Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/18550 (epoch 0), train_loss = 1.915, time/batch = 0.482\n",
      ">> sample mode:\n",
      "The aut not he sol. Ged I my them Satel?\n",
      "She chise'd t\n",
      "----------------------------------\n",
      "741/18550 (epoch 1), train_loss = 1.748, time/batch = 0.462\n",
      ">> sample mode:\n",
      "The wriep asse: boot begiend.\n",
      "\n",
      "ANFERIO:\n",
      "Wes but frien:\n",
      "----------------------------------\n",
      "1112/18550 (epoch 2), train_loss = 1.669, time/batch = 0.480\n",
      ">> sample mode:\n",
      "The Sid life?\n",
      "\n",
      "ASTINGWARKATLANO:\n",
      "Whone her.\n",
      "\n",
      "KING EDWA\n",
      "----------------------------------\n",
      "1483/18550 (epoch 3), train_loss = 1.626, time/batch = 0.559\n",
      ">> sample mode:\n",
      "The stily die.\n",
      "\n",
      "CLARENCE:\n",
      "Lo is a hogrown,\n",
      "In ever,\n",
      "An\n",
      "----------------------------------\n",
      "1854/18550 (epoch 4), train_loss = 1.598, time/batch = 0.503\n",
      ">> sample mode:\n",
      "The raign lets aftere\n",
      "I hole foreet comes that behoked\n",
      "----------------------------------\n",
      "2225/18550 (epoch 5), train_loss = 1.575, time/batch = 0.541\n",
      ">> sample mode:\n",
      "The was to come; bid Cade:\n",
      "And I'll neweds an Forth\n",
      "Do\n",
      "----------------------------------\n",
      "2596/18550 (epoch 6), train_loss = 1.556, time/batch = 0.501\n",
      ">> sample mode:\n",
      "The father of Jully hath cannot both come flowice if c\n",
      "----------------------------------\n",
      "2967/18550 (epoch 7), train_loss = 1.542, time/batch = 0.502\n",
      ">> sample mode:\n",
      "The seen stit, peace,\n",
      "Led in all to with wirther of yo\n",
      "----------------------------------\n",
      "3338/18550 (epoch 8), train_loss = 1.531, time/batch = 0.485\n",
      ">> sample mode:\n",
      "The of the Cature of unjust no queen heaven him themse\n",
      "----------------------------------\n",
      "3709/18550 (epoch 9), train_loss = 1.522, time/batch = 0.501\n",
      ">> sample mode:\n",
      "The let'st ourily, sir, 'tis not sweet in my wife? thy\n",
      "----------------------------------\n",
      "4080/18550 (epoch 10), train_loss = 1.515, time/batch = 0.484\n",
      ">> sample mode:\n",
      "The palment; the come,\n",
      "For vabut of\n",
      "uncle, sleep\n",
      "In ch\n",
      "----------------------------------\n",
      "4451/18550 (epoch 11), train_loss = 1.508, time/batch = 0.518\n",
      ">> sample mode:\n",
      "The ofterethings with my death, and your highly before\n",
      "----------------------------------\n",
      "4822/18550 (epoch 12), train_loss = 1.502, time/batch = 0.504\n",
      ">> sample mode:\n",
      "The whost, blesse, to indeed.\n",
      "\n",
      "STANRECO:\n",
      "Marisa, there\n",
      "----------------------------------\n",
      "5193/18550 (epoch 13), train_loss = 1.496, time/batch = 0.502\n",
      ">> sample mode:\n",
      "The believe sons,\n",
      "For innoulbs;\n",
      "Untending leave, yet s\n",
      "----------------------------------\n",
      "5564/18550 (epoch 14), train_loss = 1.492, time/batch = 0.518\n",
      ">> sample mode:\n",
      "The way;'\n",
      "And he need;\n",
      "And deaded nor Edward mening so\n",
      "----------------------------------\n",
      "5935/18550 (epoch 15), train_loss = 1.488, time/batch = 0.518\n",
      ">> sample mode:\n",
      "The frayeul.\n",
      "\n",
      "First Lord:\n",
      "An my more to but, here do I\n",
      "----------------------------------\n",
      "6306/18550 (epoch 16), train_loss = 1.485, time/batch = 0.540\n",
      ">> sample mode:\n",
      "The traped, I can is your accocauge shall remokes cent\n",
      "----------------------------------\n",
      "6677/18550 (epoch 17), train_loss = 1.481, time/batch = 0.499\n",
      ">> sample mode:\n",
      "The foot.\n",
      "\n",
      "CORIOLANUS:\n",
      "Pray you sclaice. Warl: it thou\n",
      "----------------------------------\n",
      "7048/18550 (epoch 18), train_loss = 1.478, time/batch = 0.538\n",
      ">> sample mode:\n",
      "The Duke cannot of stor a thousands, in his duke of th\n",
      "----------------------------------\n",
      "7419/18550 (epoch 19), train_loss = 1.475, time/batch = 0.522\n",
      ">> sample mode:\n",
      "The cheer him; and nor I be dring,--any in poinows tho\n",
      "----------------------------------\n",
      "7790/18550 (epoch 20), train_loss = 1.472, time/batch = 0.478\n",
      ">> sample mode:\n",
      "The busier, good war should she should datures' will o\n",
      "----------------------------------\n",
      "8161/18550 (epoch 21), train_loss = 1.470, time/batch = 0.538\n",
      ">> sample mode:\n",
      "The very master arging ulself\n",
      "To both her die,\n",
      "One pos\n",
      "----------------------------------\n",
      "8532/18550 (epoch 22), train_loss = 1.467, time/batch = 0.559\n",
      ">> sample mode:\n",
      "The wrate all charied:\n",
      "From Plear wedke to a villan he\n",
      "----------------------------------\n",
      "8903/18550 (epoch 23), train_loss = 1.465, time/batch = 0.539\n",
      ">> sample mode:\n",
      "The ofacubles,\n",
      "To trowas,\n",
      "A'kemb us,\n",
      "kid the exekes yo\n",
      "----------------------------------\n",
      "9274/18550 (epoch 24), train_loss = 1.464, time/batch = 0.517\n",
      ">> sample mode:\n",
      "The made a dread me it, but he once out assallow spro.\n",
      "----------------------------------\n",
      "9645/18550 (epoch 25), train_loss = 1.462, time/batch = 0.522\n",
      ">> sample mode:\n",
      "The first do too sept young of TEl:\n",
      "Deserancory else, \n",
      "----------------------------------\n",
      "10016/18550 (epoch 26), train_loss = 1.460, time/batch = 0.459\n",
      ">> sample mode:\n",
      "The harms,\n",
      "Buried a king:\n",
      "Cutigdly meet the most pains\n",
      "----------------------------------\n",
      "10387/18550 (epoch 27), train_loss = 1.459, time/batch = 0.542\n",
      ">> sample mode:\n",
      "The streful; I do not no one too body as punish not of\n",
      "----------------------------------\n",
      "10758/18550 (epoch 28), train_loss = 1.458, time/batch = 0.540\n",
      ">> sample mode:\n",
      "The terroul;\n",
      "For I cheary to thy wife\n",
      "With out\n",
      "And int\n",
      "----------------------------------\n",
      "11129/18550 (epoch 29), train_loss = 1.457, time/batch = 0.500\n",
      ">> sample mode:\n",
      "The unthound,\n",
      "I knows or our safel of many men you;\n",
      "We\n",
      "----------------------------------\n",
      "11500/18550 (epoch 30), train_loss = 1.455, time/batch = 0.500\n",
      ">> sample mode:\n",
      "The eyeshop, to court before her remous\n",
      "Things, you so\n",
      "----------------------------------\n",
      "11871/18550 (epoch 31), train_loss = 1.454, time/batch = 0.518\n",
      ">> sample mode:\n",
      "The puining this grie--\n",
      "\n",
      "KATHARINA:\n",
      "Neither bletch;\n",
      "As\n",
      "----------------------------------\n",
      "12242/18550 (epoch 32), train_loss = 1.453, time/batch = 0.517\n",
      ">> sample mode:\n",
      "The widouse to the gods, sir, for them after's, Seems,\n",
      "----------------------------------\n",
      "12613/18550 (epoch 33), train_loss = 1.452, time/batch = 0.519\n",
      ">> sample mode:\n",
      "The curse,\n",
      "The way content us injurks and liel, a peer\n",
      "----------------------------------\n",
      "12984/18550 (epoch 34), train_loss = 1.451, time/batch = 0.483\n",
      ">> sample mode:\n",
      "The was, to a hationt no back'd belose England: you pr\n",
      "----------------------------------\n",
      "13355/18550 (epoch 35), train_loss = 1.449, time/batch = 0.478\n",
      ">> sample mode:\n",
      "The offencu of on turn.\n",
      "\n",
      "WARWICK:\n",
      "And it with wife.\n",
      "\n",
      "S\n",
      "----------------------------------\n",
      "13726/18550 (epoch 36), train_loss = 1.448, time/batch = 0.500\n",
      ">> sample mode:\n",
      "The war of his confionl knaff were to give him? insolu\n",
      "----------------------------------\n",
      "14097/18550 (epoch 37), train_loss = 1.447, time/batch = 0.499\n",
      ">> sample mode:\n",
      "The dough, if lady-surmal, warm.\n",
      "\n",
      "YORK:\n",
      "You those gow'\n",
      "----------------------------------\n",
      "14468/18550 (epoch 38), train_loss = 1.446, time/batch = 0.563\n",
      ">> sample mode:\n",
      "The else leign content me,\n",
      "Or did,\n",
      "Farther three it, b\n",
      "----------------------------------\n",
      "14839/18550 (epoch 39), train_loss = 1.445, time/batch = 0.839\n",
      ">> sample mode:\n",
      "The is hour intaly,\n",
      "You are a worse you, to fedke?\n",
      "\n",
      "AN\n",
      "----------------------------------\n",
      "15210/18550 (epoch 40), train_loss = 1.444, time/batch = 0.483\n",
      ">> sample mode:\n",
      "The one me,--\n",
      "\n",
      "JUns:\n",
      "Our to sent it-on, the bless door\n",
      "----------------------------------\n",
      "15581/18550 (epoch 41), train_loss = 1.442, time/batch = 0.521\n",
      ">> sample mode:\n",
      "The out with Yorked and made mine one heavens,\n",
      "But lik\n",
      "----------------------------------\n",
      "15952/18550 (epoch 42), train_loss = 1.441, time/batch = 0.518\n",
      ">> sample mode:\n",
      "The prebite weep, but this death.\n",
      "3 KING HENRY VI:\n",
      "\n",
      "GO\n",
      "----------------------------------\n",
      "16323/18550 (epoch 43), train_loss = 1.440, time/batch = 0.501\n",
      ">> sample mode:\n",
      "The aid\n",
      "ackest unlive.\n",
      "\n",
      "POMPEY:\n",
      "How\n",
      "Elself-hoes, speak\n",
      "----------------------------------\n",
      "16694/18550 (epoch 44), train_loss = 1.439, time/batch = 0.492\n",
      ">> sample mode:\n",
      "The blessed is since you tellery to thee;\n",
      "And these sw\n",
      "----------------------------------\n",
      "17065/18550 (epoch 45), train_loss = 1.438, time/batch = 0.517\n",
      ">> sample mode:\n",
      "The griefle with his head. O here,\n",
      "Have I an head saw \n",
      "----------------------------------\n",
      "17436/18550 (epoch 46), train_loss = 1.437, time/batch = 0.457\n",
      ">> sample mode:\n",
      "The no less! we the young on eyes,\n",
      "You sweet arm, the \n",
      "----------------------------------\n",
      "17807/18550 (epoch 47), train_loss = 1.436, time/batch = 0.499\n",
      ">> sample mode:\n",
      "The was fortunes\n",
      "Will becoms\n",
      "the\n",
      "besper to go along gr\n",
      "----------------------------------\n",
      "18178/18550 (epoch 48), train_loss = 1.436, time/batch = 0.671\n",
      ">> sample mode:\n",
      "The charge home, and seek, yet you show use that I cam\n",
      "----------------------------------\n",
      "18549/18550 (epoch 49), train_loss = 1.435, time/batch = 0.502\n",
      ">> sample mode:\n",
      "The ofactome upon you.\n",
      "O, enough this hath might that \n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        data_loader.reset_batch_pointer()\n",
    "        state = sess.run(model.initial_state) # (2x[60x128])\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            x, y = data_loader.next_batch()\n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
    "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "            end = time.time()\n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        with tf.variable_scope(\"rnn\", reuse=True):\n",
    "            sample_model = LSTMModel(sample=True)\n",
    "            print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=50, prime='The ', sampling_type=1))\n",
    "            print ('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
